{
  "name": "bedrock-proxy-endpoint",
  "version": "2.5.0",
  "description": "ðŸ”€ Bedrock Proxy Endpoint â‡¢ Spin up your own custom OpenAI API server endpoint for easy AWS Bedrock inference (using standard `baseUrl`, and `apiKey` params)",
  "main": "server.js",
  "type": "module",
  "scripts": {
    "clean": "npx rimraf node_modules && npx rimraf package-lock.json && npm install",
    "server": "node --no-deprecation server.js",
    "start": "node --no-deprecation server.js",
    "test": "vitest",
    "test:run": "vitest run",
    "test:watch": "vitest --watch",
    "test:coverage": "vitest run --coverage",
    "test:ui": "vitest --ui",
    "prepare": "husky",
    "docker:build": "docker build -t bedrock-proxy-endpoint .",
    "docker:clean": "docker stop bedrock-proxy-endpoint 2>/dev/null || true && docker rm bedrock-proxy-endpoint 2>/dev/null || true",
    "docker:pull": "docker pull ghcr.io/jparkerweb/bedrock-proxy-endpoint:latest",
    "docker:run:ghcr": "docker run -d --name bedrock-proxy-endpoint -p 88:88 --env-file .env ghcr.io/jparkerweb/bedrock-proxy-endpoint:latest",
    "docker:run": "docker run -d --name bedrock-proxy-endpoint -p 88:88 --env-file .env bedrock-proxy-endpoint",
    "docker:run:dev": "docker run -it --rm --name bedrock-proxy-endpoint-dev -p 88:88 --env-file .env bedrock-proxy-endpoint"
  },
  "keywords": [
    "openai",
    "bedrock",
    "aws",
    "proxy",
    "wrapper",
    "serverless",
    "inference",
    "llm"
  ],
  "author": "",
  "license": "ISC",
  "dependencies": {
    "bedrock-wrapper": "^2.5.0",
    "body-parser": "^1.20.3",
    "dotenv": "^17.2.2",
    "express": "^4.21.2",
    "express-rate-limit": "^7.5.1"
  },
  "devDependencies": {
    "@vitest/coverage-v8": "^3.2.4",
    "@vitest/ui": "^3.2.4",
    "husky": "^9.1.7",
    "supertest": "^7.1.4",
    "vitest": "^3.2.4"
  },
  "repository": {
    "type": "git",
    "url": "git@github.com-jparkerweb:jparkerweb/bedrock-proxy-endpoint.git"
  }
}
